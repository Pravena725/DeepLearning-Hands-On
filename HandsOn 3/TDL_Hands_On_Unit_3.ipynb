{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TDL Hands-On Unit 3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnWwhX0V_5XX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "import pickle \n",
        "import numpy as np \n",
        "import os\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = [] \n",
        "\n",
        "for i in file:\n",
        "  lines.append(i)      \n",
        "\n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1]) \n",
        "print(\"\\n\")\n",
        "\n",
        "# Cleaning data\n",
        "data = \"\"  \n",
        "\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)      \n",
        "\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360] \n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space \n",
        "\n",
        "new_data = data.translate(translator)  \n",
        "\n",
        "new_data[:500] \n",
        "\n",
        "z = [] \n",
        "for i in data.split():\n",
        "  if i not in z:\n",
        "    z.append(i)          \n",
        "\n",
        "data = ' '.join(z)\n",
        "\n",
        "data[:500] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Xiu3NiQIBjJh",
        "outputId": "3213788a-142e-4e3d-bfe7-e865269e8fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Line:  One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "The Last Line:  subscribe to our email newsletter to hear about new eBooks.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "4EPM3qasDfFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data]) \n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb')) \n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]  \n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "print(vocab_size) \n",
        "\n",
        "sequences = [] \n",
        "for i in range(1, len(sequence_data)):\n",
        "  words = sequence_data[i-1:i+1]\n",
        "  sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences)) \n",
        "\n",
        "sequences = np.array(sequences) \n",
        "\n",
        "sequences[:10] \n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "  X.append(i[0])\n",
        "  y.append(i[1]) \n",
        "\n",
        "X = np.array(X) \n",
        "y = np.array(y) \n",
        "\n",
        "print(\"The Data is: \", X[:5]) \n",
        "print(\"The responses are: \", y[:5]) \n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size) \n",
        "\n",
        "y[:5]  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True)) \n",
        "model.add(LSTM(1000)) \n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\")) \n",
        "model.summary() \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001)) \n",
        "model.fit(X, y, epochs=150, batch_size=64) \n",
        "model.save('netword1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reniUIn3DdT7",
        "outputId": "214c0565-8062-4b8b-8ecf-5e6fd57e4ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3054\n",
            "The Length of sequences are:  4685\n",
            "The Data is:  [ 20  80 353   5  29]\n",
            "The responses are:  [ 80 353   5  29 874]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             30540     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3054)              3057054   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,136,594\n",
            "Trainable params: 16,136,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 21s 225ms/step - loss: 8.0296\n",
            "Epoch 2/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 7.9954\n",
            "Epoch 3/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 7.8294\n",
            "Epoch 4/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 7.6085\n",
            "Epoch 5/150\n",
            "74/74 [==============================] - 17s 228ms/step - loss: 7.4242\n",
            "Epoch 6/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 7.3078\n",
            "Epoch 7/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 7.2114\n",
            "Epoch 8/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 7.1212\n",
            "Epoch 9/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 7.0283\n",
            "Epoch 10/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 6.9466\n",
            "Epoch 11/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 6.8508\n",
            "Epoch 12/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 6.7393\n",
            "Epoch 13/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 6.5982\n",
            "Epoch 14/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 6.4164\n",
            "Epoch 15/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 6.2193\n",
            "Epoch 16/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 6.0313\n",
            "Epoch 17/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 5.8563\n",
            "Epoch 18/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 5.6951\n",
            "Epoch 19/150\n",
            "74/74 [==============================] - 16s 220ms/step - loss: 5.5535\n",
            "Epoch 20/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 5.4256\n",
            "Epoch 21/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 5.2987\n",
            "Epoch 22/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 5.1944\n",
            "Epoch 23/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 5.0902\n",
            "Epoch 24/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 4.9561\n",
            "Epoch 25/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 4.8612\n",
            "Epoch 26/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 4.7784\n",
            "Epoch 27/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 4.7084\n",
            "Epoch 28/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 4.6185\n",
            "Epoch 29/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 4.5389\n",
            "Epoch 30/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 4.4710\n",
            "Epoch 31/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 4.3637\n",
            "Epoch 32/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 4.2946\n",
            "Epoch 33/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 4.2316\n",
            "Epoch 34/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 4.1566\n",
            "Epoch 35/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 4.0916\n",
            "Epoch 36/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 4.0071\n",
            "Epoch 37/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 3.9530\n",
            "Epoch 38/150\n",
            "74/74 [==============================] - 16s 220ms/step - loss: 3.8512\n",
            "Epoch 39/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 3.7571\n",
            "Epoch 40/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 3.6794\n",
            "Epoch 41/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 3.5929\n",
            "Epoch 42/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 3.5259\n",
            "Epoch 43/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 3.4342\n",
            "Epoch 44/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 3.3242\n",
            "Epoch 45/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 3.2268\n",
            "Epoch 46/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 3.1749\n",
            "Epoch 47/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 3.0974\n",
            "Epoch 48/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 3.0368\n",
            "Epoch 49/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 2.9921\n",
            "Epoch 50/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 2.9126\n",
            "Epoch 51/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 2.8357\n",
            "Epoch 52/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.7691\n",
            "Epoch 53/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.7222\n",
            "Epoch 54/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 2.7030\n",
            "Epoch 55/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.6435\n",
            "Epoch 56/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.6210\n",
            "Epoch 57/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.5681\n",
            "Epoch 58/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.4954\n",
            "Epoch 59/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.4634\n",
            "Epoch 60/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.4343\n",
            "Epoch 61/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.3836\n",
            "Epoch 62/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.3648\n",
            "Epoch 63/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.3267\n",
            "Epoch 64/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.3047\n",
            "Epoch 65/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.2762\n",
            "Epoch 66/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.2502\n",
            "Epoch 67/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 2.2243\n",
            "Epoch 68/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 2.1804\n",
            "Epoch 69/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 2.1628\n",
            "Epoch 70/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.1369\n",
            "Epoch 71/150\n",
            "74/74 [==============================] - 16s 220ms/step - loss: 2.0979\n",
            "Epoch 72/150\n",
            "74/74 [==============================] - 16s 220ms/step - loss: 2.0714\n",
            "Epoch 73/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 2.0578\n",
            "Epoch 74/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 2.0213\n",
            "Epoch 75/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 2.0120\n",
            "Epoch 76/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 2.0196\n",
            "Epoch 77/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 2.0061\n",
            "Epoch 78/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.9840\n",
            "Epoch 79/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.9693\n",
            "Epoch 80/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.9483\n",
            "Epoch 81/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.9139\n",
            "Epoch 82/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.8860\n",
            "Epoch 83/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.8955\n",
            "Epoch 84/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.9017\n",
            "Epoch 85/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.8345\n",
            "Epoch 86/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.8050\n",
            "Epoch 87/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.7918\n",
            "Epoch 88/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.7912\n",
            "Epoch 89/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.7677\n",
            "Epoch 90/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.7267\n",
            "Epoch 91/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.7618\n",
            "Epoch 92/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.7537\n",
            "Epoch 93/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.7160\n",
            "Epoch 94/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.6984\n",
            "Epoch 95/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.6719\n",
            "Epoch 96/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.6787\n",
            "Epoch 97/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.6860\n",
            "Epoch 98/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.6925\n",
            "Epoch 99/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 1.6683\n",
            "Epoch 100/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.6514\n",
            "Epoch 101/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.6188\n",
            "Epoch 102/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.6270\n",
            "Epoch 103/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.5971\n",
            "Epoch 104/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.5706\n",
            "Epoch 105/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 1.5961\n",
            "Epoch 106/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.5711\n",
            "Epoch 107/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.5645\n",
            "Epoch 108/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 1.5736\n",
            "Epoch 109/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.5288\n",
            "Epoch 110/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.5239\n",
            "Epoch 111/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.4906\n",
            "Epoch 112/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.5099\n",
            "Epoch 113/150\n",
            "74/74 [==============================] - 17s 229ms/step - loss: 1.4960\n",
            "Epoch 114/150\n",
            "74/74 [==============================] - 17s 229ms/step - loss: 1.4664\n",
            "Epoch 115/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.4651\n",
            "Epoch 116/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 1.4393\n",
            "Epoch 117/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 1.4320\n",
            "Epoch 118/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.4367\n",
            "Epoch 119/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.4335\n",
            "Epoch 120/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.4183\n",
            "Epoch 121/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.4161\n",
            "Epoch 122/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.4280\n",
            "Epoch 123/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.4232\n",
            "Epoch 124/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 1.4335\n",
            "Epoch 125/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.3927\n",
            "Epoch 126/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 1.3790\n",
            "Epoch 127/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.3585\n",
            "Epoch 128/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 1.3623\n",
            "Epoch 129/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.3397\n",
            "Epoch 130/150\n",
            "74/74 [==============================] - 17s 227ms/step - loss: 1.3314\n",
            "Epoch 131/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.3108\n",
            "Epoch 132/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.2808\n",
            "Epoch 133/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.2761\n",
            "Epoch 134/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.2826\n",
            "Epoch 135/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.3008\n",
            "Epoch 136/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.2827\n",
            "Epoch 137/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.2655\n",
            "Epoch 138/150\n",
            "74/74 [==============================] - 17s 224ms/step - loss: 1.2546\n",
            "Epoch 139/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.2250\n",
            "Epoch 140/150\n",
            "74/74 [==============================] - 17s 226ms/step - loss: 1.2024\n",
            "Epoch 141/150\n",
            "74/74 [==============================] - 17s 223ms/step - loss: 1.1860\n",
            "Epoch 142/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.1820\n",
            "Epoch 143/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.2061\n",
            "Epoch 144/150\n",
            "74/74 [==============================] - 17s 225ms/step - loss: 1.1974\n",
            "Epoch 145/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.1923\n",
            "Epoch 146/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.1745\n",
            "Epoch 147/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 1.1762\n",
            "Epoch 148/150\n",
            "74/74 [==============================] - 16s 221ms/step - loss: 1.1709\n",
            "Epoch 149/150\n",
            "74/74 [==============================] - 16s 222ms/step - loss: 1.1744\n",
            "Epoch 150/150\n",
            "74/74 [==============================] - 16s 223ms/step - loss: 1.1642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model \n",
        "import numpy as np \n",
        "import pickle \n",
        "\n",
        "\n",
        "# Load the model and tokenizer \n",
        "model = load_model('netword1.h5') \n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb')) \n",
        " \n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text): \n",
        "  \"\"\" In this function we are using the tokenizer and models trained\n",
        "      and we are creating the sequence of the text entered and then\n",
        "      using our model to predict and return the the predicted word.\"\"\" \n",
        "\n",
        "  for i in range(3):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    sequence = np.array(sequence) \n",
        "    preds = model.predict_classes(sequence) \n",
        "    # print(preds) \n",
        "    predicted_word = \"\" \n",
        "    for key, value in tokenizer.word_index.items():\n",
        "      if value == preds: \n",
        "        predicted_word = key\n",
        "        break\n",
        "\n",
        "    print(predicted_word) \n",
        "    return predicted_word \n",
        "\n",
        " \n",
        "\n",
        "\"\"\" We are testing our model and we will run the model\n",
        "    until the user decides to stop the script. \n",
        "    While the script is running we try and check if\n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\"\"\" \n",
        "\n",
        " \n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\" \n",
        "# text4 = \"stop the script\" \n",
        "\n",
        "\n",
        "while(True):\n",
        "  text = input(\"Enter your line: \") \n",
        "\n",
        "  if text == \"stop the script\":\n",
        "    print(\"Ending The Program.....\") \n",
        "    break \n",
        "  \n",
        "  else:\n",
        "    try: \n",
        "      text = text.split(\" \") \n",
        "      text = text[-1] \n",
        "      text = ''.join(text) \n",
        "      Predict_Next_Words(model, tokenizer, text) \n",
        "\n",
        "    except: \n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MKcr3VT-tbu",
        "outputId": "88e3d18a-4507-4a44-c052-ab140c3a95e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: collection of textile\n",
            "Enter your line: stop the script\n",
            "Ending The Program.....\n"
          ]
        }
      ]
    }
  ]
}