#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jan  3 13:13:45 2022

@author: alphago
"""

from numpy import zeros
from numpy import ones
import tensorflow as tf
import tensorflow.keras
import numpy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten, MaxPooling2D, Concatenate, Reshape, Conv2DTranspose, Input
from tensorflow.keras.optimizers import Adam
import os
from sklearn.metrics import accuracy_score
import cv2
import numpy as np
import matplotlib.pyplot as plt

import random
from tensorflow.keras.initializers import RandomNormal


def prepare_target_dataset(img_folder):
    img_data = []
    #for dir1 in os.listdir(img_folder): 
    for file in os.listdir(img_folder): 
        image_path= os.path.join(img_folder, file)
        #print(image_path)
        image= plt.imread(image_path)
        image = tf.image.rgb_to_grayscale(image)
        image = tensorflow.image.resize(image,(32,32))
        image=np.array(image)
        image = image.astype('float32')
        image /= 255 
        img_data.append(image)    
    return np.array(img_data)


# select real samples
def generate_real_samples(dataset, n_samples):
    images = dataset
    #print("inside target samples")
    ix = random.sample(range(0,images.shape[0]), n_samples)
    X = []
    for i in ix:
        image = images[i]
        X.append(image)
        # generate class labels
        y = np.ones(n_samples)
    X = np.array(X)     
    X = X.reshape(n_samples, 32, 32, 1)   
    print(np.shape(X), np.shape(y))
    return X, y

# generate points in latent space as input for the generator
def generate_latent_points(latent_dim, n_samples):
    # generate points in the latent space
    x_input = numpy.random.randn(latent_dim * n_samples)
    # reshape into a batch of inputs for the network
    x_input = x_input.reshape(n_samples, latent_dim)
    return x_input


# use the generator to generate n fake examples, with class labels
def generate_fake_samples(g_model, latent_dim, n_samples):
    # generate points in latent space
    x_input = generate_latent_points(latent_dim, n_samples)
    # predict outputs
    X = g_model.predict(x_input)
    # create ✬ fake ✬ class labels (0)
    y = np.zeros(n_samples)
    print(np.shape(X), np.shape(y))
    return X, y

# define the standalone discriminator model
def define_discriminator(in_shape=(32,32,1)):
    model = Sequential()
    # normal
    model.add(Conv2D(64, (3,3), padding= "same" , input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    # downsample
    model.add(Conv2D(128, (3,3), strides=(2,2), padding= "same"))
    model.add(LeakyReLU(alpha=0.2))
    # downsample
    model.add(Conv2D(128, (3,3), strides=(2,2), padding= "same"))
    model.add(LeakyReLU(alpha=0.2))
    # downsample
    #model.add(Conv2D(256, (3,3), strides=(2,2), padding= "same"))
    #model.add(LeakyReLU(alpha=0.2))
    # classifier
    model.add(Flatten())
    #model.add(Dropout(0.4))
    model.add(Dense(1, activation= "sigmoid"))
    # compile model
    opt = Adam(lr=0.0002, beta_1=0.5)
    model.compile(loss= "binary_crossentropy" , optimizer=opt, metrics=["accuracy"])
    print(model.summary())
    return model

def define_generator(latent_dim):
    model = Sequential()
    # foundation for 4x4 image
    n_nodes = 256 * 4 * 4
    model.add(Dense(n_nodes, input_dim=latent_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Reshape((4, 4, 256)))
    # upsample to 8x8
    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding= "same"))
    model.add(LeakyReLU(alpha=0.2))
    # upsample to 16x16
    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding= "same"))
    model.add(LeakyReLU(alpha=0.2))
    # upsample to 32x32
    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding= "same"))
    model.add(LeakyReLU(alpha=0.2))
    # output layer
    model.add(Conv2D(1, (3,3), activation= "tanh" , padding= "same" ))
    print(model.summary())
    return model

# define the combined generator and discriminator model, for updating the generator
def define_gan(g_model, d_model, latent_dim):
    # make weights in the discriminator not trainable
    print("inside GAN")
    d_model.trainable = False
    # connect them
    model = Sequential()
    #model.add(Input(shape=latent_dim))
    # add generator
    model.add(g_model)
    # add the discriminator
    model.add(d_model)
    # compile model
    opt = Adam(lr=0.0002, beta_1=0.5)
    model.compile(loss= "binary_crossentropy" , optimizer=opt)
    return model

def plot_history(d1,d2,g):
    plt.figure(figsize=(12,3))
    #plt.subplot(2, 1, 1)
    plt.title(label="Accumulated Losses of Discriminator and Generator", fontsize=13)
    plt.xlabel("Epochs")
    plt.plot(d1, label='domain-loss')
    plt.plot(d2, label='domain-loss_target')
    plt.plot(g, label = 'generator_loss')
    plt.legend()
    plt.show()
    
	# save plot to file
	#pyplot.savefig('results_baseline/plot_line_plot_loss.png')
    plt.close()
    return   

    # create and save a plot of generated images
    
def save_plot(examples, epoch, n=7):
    # scale from [-1,1] to [0,1]
    examples = (examples + 1) / 2.0
    print(np.shape(examples))
    # plot images
    for i in range(n * n):
        # define subplot
        plt.subplot(n, n, 1 + i)
        # turn off axis
        plt.axis("off")
        # plot raw pixel data
        
        plt.imshow(np.squeeze(examples[i]))
        # save plot to file
        filename = "generated_plot_e%03d.png" % (epoch+1)
        plt.savefig(filename)
        plt.close()
        # evaluate the discriminator, plot generated images, save generator model
        
def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):
    # prepare real samples
    X_real, y_real = generate_real_samples(dataset, n_samples)
    # evaluate discriminator on real examples
    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)
    # prepare fake examples
    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)
    
    # evaluate discriminator on fake examples
    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)
    # summarize discriminator performance
    print(" >Accuracy real: %.0f%%, fake: %.0f%%" % (acc_real*100, acc_fake*100))
    # save plot
    save_plot(x_fake, epoch)
    # save the generator model tile file
    filename = "generator_model_%03d.h5" % (epoch+1)
    g_model.save(filename)
    # train the generator and discriminator
    
def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=700, n_batch=128):
    bat_per_epo = int(dataset.shape[0] / n_batch)
    half_batch = int(n_batch / 2)
    d1_hist, d2_hist,g_hist = list(), list(), list()
    # manually enumerate epochs
    for i in range(n_epochs):
        # enumerate batches over the training set
        for j in range(bat_per_epo):
            # get randomly selected ✬ real ✬ samples
            X_real, y_real = generate_real_samples(dataset, half_batch)
            # update discriminator model weights
            d_loss1, _ = d_model.train_on_batch(X_real, y_real)
            # generate ✬ fake ✬ examples
            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
            # update discriminator model weights
            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)
            # prepare points in latent space as input for the generator
            X_gan = generate_latent_points(latent_dim, n_batch)
            # create inverted labels for the fake samples
            y_gan = np.ones(n_batch)
            # update the generator via the discriminator ✬ s error
            g_loss = gan_model.train_on_batch(X_gan, y_gan)
            # summarize loss on this batch  , 
            print(" >%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f " % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))
            d1_hist.append(d_loss1)
            d2_hist.append(d_loss2)
            g_hist.append(g_loss)
            # evaluate the model performance, sometimes
            if (i+1) % 10 == 0:
                print(np.shape(dataset))
                summarize_performance(i, g_model, d_model, dataset, latent_dim)
                plot_history(d1_hist, d2_hist, g_hist)
    return

# load image data
train_Xtarget = prepare_target_dataset(r'/home/alphago/Asha/train')
# size of the latent space
latent_dim = 100
# create the discriminator
d_model = define_discriminator()
print("discriminator created")
# create the generator
g_model = define_generator(latent_dim)
print("generator created")
# create the gan
gan_model = define_gan(g_model, d_model, latent_dim)
print("GAN created")

# train model
train(g_model, d_model, gan_model, train_Xtarget, latent_dim)